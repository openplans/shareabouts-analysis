{
 "metadata": {
  "name": "",
  "signature": "sha256:f61dd99b4e56d266090a0a1423e95ffc6509952c79255656be4f038e2a137df8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# helpers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import csv\n",
      "import os\n",
      "import time\n",
      "from time import sleep\n",
      "import requests\n",
      "from requests.auth import HTTPBasicAuth\n",
      "\n",
      "MY_USERNAME = \"afraint\"\n",
      "MY_PASSWORD = \"Tomorrownow!\"\n",
      "\n",
      "def get_data(URL, retries=5):\n",
      "    '''   go to the URL and return the JSON using my credentials  '''\n",
      "    try:\n",
      "        r = requests.get(URL, auth=HTTPBasicAuth(MY_USERNAME, MY_PASSWORD))\n",
      "    except Exception as e:\n",
      "        print 'Something went wrong: %s - %s\\r' % (type(e).__name__, e),\n",
      "    else:\n",
      "        if r.status_code == 200:\n",
      "            return r.json()\n",
      "        else:\n",
      "            print 'Got an unexpected %s response: %s\\r' % (r.status_code, r.text),\n",
      "\n",
      "    if retries > 0:\n",
      "        sleep(1)\n",
      "        return get_data(URL, retries - 1)\n",
      "    else:\n",
      "        print '\\nNo more retries for URL \"%s\".' % (URL,)\n",
      "        sys.exit(1)\n",
      "\n",
      "def pull_dataset_data(JSON, data_node):\n",
      "    ''' handles pagination. dump the data into a list, and repeat if there's another page   '''\n",
      "    results = []\n",
      "    while JSON[\"metadata\"][\"next\"] is not None:     # if there's another page\n",
      "        for item in JSON[data_node]:\n",
      "            results.append(item)\n",
      "        JSON = get_data(JSON[\"metadata\"][\"next\"])\n",
      "    for item in JSON[data_node]:                    # dump the data one more time for the final page\n",
      "        results.append(item)\n",
      "    return results\n",
      "\n",
      "def get_dataset_history(PLACES_URL):\n",
      "    ''' identifies the first and last date that places were added to the dataset   '''\n",
      "    print PLACES_URL\n",
      "    JSON = get_data(PLACES_URL)\n",
      "    if JSON[\"metadata\"][\"length\"] > 200000:\n",
      "        return None, None\n",
      "    else:\n",
      "        all_places = pull_dataset_data(JSON, \"features\")\n",
      "        all_dates = []\n",
      "        for place in all_places:\n",
      "            created = str(place[\"properties\"][\"created_datetime\"].split(\"T\")[0])\n",
      "            if created not in all_dates:\n",
      "                all_dates.append(created)\n",
      "        if len(all_dates) > 0:\n",
      "            first_place = sorted(all_dates)[0]\n",
      "            last_place = sorted(all_dates)[-1:][0]\n",
      "            return first_place, last_place\n",
      "        else:\n",
      "            return None, None\n",
      "\n",
      "\n",
      "def pull_relevant_data(JSON):\n",
      "    '''  this is where the data of value is pulled out of a dictionary and tossed into a list  '''\n",
      "    results = []\n",
      "    for dataset in JSON:\n",
      "        # get the number and type of non-point user interaction\n",
      "        comments = 0\n",
      "        type_of_interaction = []\n",
      "        for item in dataset[\"submission_sets\"]:\n",
      "            comments += dataset[\"submission_sets\"][item][\"length\"]\n",
      "            type_of_interaction.append(str(item))\n",
      "        # isolate the username and dump the data into a list\n",
      "        owner_name = str(dataset[\"owner\"].split(\"/\")[-1:][0] )\n",
      "\n",
      "        first_point, last_point = get_dataset_history(dataset[\"places\"][\"url\"])\n",
      "\n",
      "        data = [ owner_name.encode('utf8'),               # 0\n",
      "                 dataset[\"owner\"].encode('utf8'),         # 1\n",
      "                 dataset[\"display_name\"].encode('utf8'),  # 2\n",
      "                 dataset[\"slug\"].encode('utf8'),          # 3\n",
      "                 dataset[\"places\"][\"length\"],             # 4\n",
      "                 dataset[\"places\"][\"url\"],                # 5\n",
      "                 comments, type_of_interaction,          # 6, 7\n",
      "                 first_point, last_point  ]               # 8, 9\n",
      "        results.append( data )\n",
      "    return results\n",
      "\n",
      "def write_csv(OUTPUT_FILE, HEADER, DATA):\n",
      "    # DATA is a dictionary\n",
      "    # HEADER is a list\n",
      "    with open(OUTPUT_FILE, 'wb') as csvfile:\n",
      "        writer = csv.writer(csvfile, delimiter=\",\")\n",
      "        writer.writerow(HEADER)\n",
      "        for item in DATA:\n",
      "            writer.writerow(DATA[item])\n",
      "    print \"Wrote file to %s\" % OUTPUT_FILE\n",
      "\n",
      "print \"Loaded all modules and helper functions\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loaded all modules and helper functions\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# pull all the emails out of the CrashStories data for TA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# make sure to \"include_private\"!\n",
      "url = 'http://api.shareabouts.org/api/v2/afraint/datasets/new_york_data/places?format=json&include_private'\n",
      "\n",
      "crash_data = pull_dataset_data(get_data(url), \"features\")\n",
      "\n",
      "unique_emails = {}\n",
      "\n",
      "for crash in crash_data:\n",
      "    try: \n",
      "        their_email = crash[\"properties\"][\"private-email\"]\n",
      "        if their_email not in unique_emails:\n",
      "            unique_emails[their_email] = [their_email]\n",
      "    except: pass\n",
      "\n",
      "output_file = os.path.join(os.getcwd(),\n",
      "                           \"exports\",\n",
      "                           \"crashstories.csv\")\n",
      "\n",
      "print len(unique_emails)\n",
      "\n",
      "write_csv(output_file, [\"email\"], unique_emails)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "125\n",
        "Wrote file to C:\\Users\\Aaron\\Documents\\IPython Notebooks\\exports\\crashstories.csv\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}