{
 "metadata": {
  "name": "",
  "signature": "sha256:e2fff76b3cf66da566df32c14ce5469e10ea1c9678f15052b417ea44ecc255bc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Exploring Shareabouts data with iPython\n",
      "### boilerplate code written to access the data catalog"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#####setup // define some helper functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "import os\n",
      "import requests\n",
      "from requests.auth import HTTPBasicAuth\n",
      "\n",
      "MY_USERNAME = \"afraint\"\n",
      "MY_PASSWORD = \"Tomorrownow!\"\n",
      "\n",
      "def get_data(URL):\n",
      "    '''   go to the URL and return the JSON using my credentials  '''\n",
      "    r = requests.get(URL, auth=HTTPBasicAuth(MY_USERNAME, MY_PASSWORD))\n",
      "    return r.json()\n",
      "\n",
      "def pull_dataset_data(JSON):\n",
      "    ''' handles pagination. dump the data into a list, and repeat if there's another page   '''\n",
      "    results = []\n",
      "    while JSON[\"metadata\"][\"next\"] is not None:     # if there's another page\n",
      "        for item in JSON[\"results\"]:\n",
      "            results.append(item)\n",
      "        JSON = get_data(JSON[\"metadata\"][\"next\"])\n",
      "    for item in JSON[\"results\"]:                    # dump the data one more time for the final page\n",
      "        results.append(item)    \n",
      "    return results\n",
      "\n",
      "def pull_relevant_data(JSON):\n",
      "    '''  this is where the data of value is pulled out of a dictionary and tossed into a list  '''\n",
      "    results = []\n",
      "    for dataset in JSON:\n",
      "        # get the number and type of non-point user interaction\n",
      "        comments = 0\n",
      "        type_of_interaction = []\n",
      "        for item in dataset[\"submission_sets\"]:\n",
      "            comments += dataset[\"submission_sets\"][item][\"length\"]\n",
      "            type_of_interaction.append(str(item))\n",
      "        # isolate the username and dump the data into a list\n",
      "        owner_name = str(dataset[\"owner\"].split(\"/\")[-1:][0] )\n",
      "        data = [ owner_name.encode('utf8'),               # 0\n",
      "                 dataset[\"owner\"].encode('utf8'),         # 1    \n",
      "                 dataset[\"display_name\"].encode('utf8'),  # 2\n",
      "                 dataset[\"slug\"].encode('utf8'),          # 3 \n",
      "                 dataset[\"places\"][\"length\"],             # 4\n",
      "                 dataset[\"places\"][\"url\"],                # 5\n",
      "                 comments, type_of_interaction ]          # 6, 7\n",
      "        results.append( data )\n",
      "    return results\n",
      "\n",
      "print \"Loaded all modules and helper functions\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loaded all modules and helper functions\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#####basic descriptions of the contents of the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_server = get_data('http://data.shareabouts.org/api/v2/~/datasets?format=json&page=1')\n",
      "api_server = get_data('http://api.shareabouts.org/api/v2/~/datasets?format=json&page=1')\n",
      "print \"METADATA CATEGORIES\"\n",
      "for item in data_server[\"metadata\"]: print \"\\t\", item\n",
      "print \"\\nRESULTS CATEGORIES\"\n",
      "for item in data_server[\"results\"][0]: print \"\\t\", item"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "METADATA CATEGORIES\n",
        "\tlength\n",
        "\tprevious\n",
        "\tpage\n",
        "\tnext\n",
        "\n",
        "RESULTS CATEGORIES\n",
        "\tdisplay_name\n",
        "\tplaces\n",
        "\turl\n",
        "\tsubmission_sets\n",
        "\tid\n",
        "\tkeys\n",
        "\towner\n",
        "\tslug\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#####confirm that my list of data matches the length of the metadata length"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_server = get_data('http://data.shareabouts.org/api/v2/~/datasets?format=json&page=1')\n",
      "api_server = get_data('http://api.shareabouts.org/api/v2/~/datasets?format=json&page=1')\n",
      "\n",
      "DATA_datasets = pull_dataset_data(data_server)\n",
      "API_datasets = pull_dataset_data(api_server)\n",
      "if len(DATA_datasets) != data_server[\"metadata\"][\"length\"]: print \"Error with DATA server data\"\n",
      "else: print \"DATA server: %i datasets\" % len(DATA_datasets)\n",
      "if len(API_datasets) != api_server[\"metadata\"][\"length\"]: print \"Error with API server data\"\n",
      "else: print \"API server: %i datasets\" % len(API_datasets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DATA server: 433 datasets\n",
        "API server: 191 datasets\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#####combine the datasets from the API and DATA servers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "API_data = pull_relevant_data(API_datasets)\n",
      "DATA_data = pull_relevant_data(DATA_datasets)\n",
      "\n",
      "all_data = {}\n",
      "\n",
      "def combine_datasets(DATA_LIST, OUTPUT_DICT):\n",
      "    '''  toss datasets into a dictionary. \n",
      "         if it's already in in dictionary, use the one with more points or comments    '''\n",
      "    for item in DATA_LIST:\n",
      "        unique_id = os.path.join(item[0], item[3]) # owner/slug\n",
      "        # if the dataset is missing from the list, add it\n",
      "        if unique_id not in OUTPUT_DICT:\n",
      "            OUTPUT_DICT[unique_id] = item\n",
      "        # if it's already in the list, use the one with more places\n",
      "        else:\n",
      "            new_case = item[5].split(\".shareabouts.org\")[0]\n",
      "            existing_case = OUTPUT_DICT[unique_id][5].split(\".shareabouts.org\")[0]\n",
      "            # PLACES\n",
      "            if item[4] > OUTPUT_DICT[unique_id][4]:\n",
      "                OUTPUT_DICT[unique_id] = item\n",
      "            elif item[4] < OUTPUT_DICT[unique_id][4]:\n",
      "                pass\n",
      "            # if they have the same number of points, check comments\n",
      "            else:\n",
      "                # use the new one if it has more comments\n",
      "                # COMMENTS\n",
      "                if item[6] > OUTPUT_DICT[unique_id][6]:\n",
      "                    OUTPUT_DICT[unique_id] = item\n",
      "                else:\n",
      "                    pass \n",
      "combine_datasets(DATA_data, all_data)\n",
      "combine_datasets(API_data, all_data)\n",
      "print len(all_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "471\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#####write the results to CSV"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output_file = os.path.join(os.getcwd(), \n",
      "                           \"shareabouts_analysis\",\n",
      "                           \"all_datasets.csv\")\n",
      "def write_csv(OUTPUT_FILE, HEADER, DATA):\n",
      "    with open(OUTPUT_FILE, 'wb') as csvfile:\n",
      "        writer = csv.writer(csvfile, delimiter=\",\")\n",
      "        writer.writerow(HEADER)\n",
      "        for item in DATA:\n",
      "            writer.writerow(DATA[item])\n",
      "    print \"Wrote file to %s\" % output_file            \n",
      "\n",
      "write_csv(output_file, \n",
      "          [\"owner\", \"owner_url\", \"display_name\", \"slug\", \"places\", \"place_URL\", \"comments\", \"comment_type\"], \n",
      "          all_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wrote file to C:\\Users\\Aaron\\Documents\\IPython Notebooks\\shareabouts_analysis\\all_datasets.csv\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# now it's time to answer some specific questions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###How many maps does each user have?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mappers = {}\n",
      "for ID in all_data:\n",
      "    user = all_data[ID][0]\n",
      "    # if the user is already in the list of mappers, add to the existing numbers\n",
      "    try: \n",
      "        mappers[user][1] += 1\n",
      "        mappers[user][2] += all_data[ID][4]\n",
      "        mappers[user][3] += all_data[ID][6]\n",
      "        mappers[user][4].append( all_data[ID][3] )\n",
      "    # if they're not in the list, add them\n",
      "    except: \n",
      "        mappers[user] = [user, \n",
      "                         1,                    # start to count the number of datasets\n",
      "                         all_data[ID][4],      # number of places\n",
      "                         all_data[ID][6],      # number of comments\n",
      "                         [all_data[ID][3] ] ]  # list of dataset names\n",
      "\n",
      "output_file = os.path.join( os.getcwd(), \"shareabouts_analysis\", \"user_info.csv\")  \n",
      "write_csv(output_file,\n",
      "          ['user', 'total_datasets','places','comments', 'dataset_names'],\n",
      "          mappers)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wrote file to C:\\Users\\Aaron\\Documents\\IPython Notebooks\\shareabouts_analysis\\user_info.csv\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Dataset activity: first point, last point"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}